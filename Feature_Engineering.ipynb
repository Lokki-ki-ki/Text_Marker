{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Project Group 5**\n",
        "### *Linguistic: Your auto essay grading tool*"
      ],
      "metadata": {
        "id": "5ky25q9H4k0n"
      },
      "id": "5ky25q9H4k0n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Load Library**"
      ],
      "metadata": {
        "id": "QgMSNzbh50o7"
      },
      "id": "QgMSNzbh50o7"
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install language_tool_python\n",
        " !pip install textstat\n",
        " !pip install textacy\n",
        " !pip install pyspellchecker\n",
        " !pip install Jamspell\n",
        " !pip install Symspellpy\n",
        " !pip install Textblob \n",
        " !pip install gensim\n",
        " !pip install lexrank\n",
        " nltk.download('punkt')\n",
        " nltk.download('stopwords')\n",
        " nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "rXyYq70aAAwZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3397a0bc-853a-47bb-9876-e7de7feb1771"
      },
      "id": "rXyYq70aAAwZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from language_tool_python) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->language_tool_python) (2.10)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 28.5 MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.13.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 57.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.13.0 textstat-0.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textacy\n",
            "  Downloading textacy-0.11.0-py3-none-any.whl (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 16.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.21.6)\n",
            "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (0.13.0)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (3.4.2)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.64.1)\n",
            "Collecting jellyfish>=0.8.0\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 62.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (2.6.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.7.3)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (4.2.4)\n",
            "Collecting cytoolz>=0.10.1\n",
            "  Downloading cytoolz-0.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 64.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from textacy) (1.0.2)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->textacy) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.0->textacy) (3.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (4.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.6.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (8.1.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (3.0.10)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (1.10.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.0->textacy) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->textacy) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.0.0->textacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy) (5.2.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->textacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->textacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0.0->textacy) (2.0.1)\n",
            "Building wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=74007 sha256=aa0a2abc58fb7972848b9c20a82780c864b85487049572502d5e3e8713ac4cd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: jellyfish, cytoolz, textacy\n",
            "Successfully installed cytoolz-0.12.0 jellyfish-0.9.0 textacy-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 27.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Jamspell\n",
            "  Downloading jamspell-0.0.12.tar.gz (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 42.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Jamspell\n",
            "  Building wheel for Jamspell (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for Jamspell\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for Jamspell\n",
            "Failed to build Jamspell\n",
            "Installing collected packages: Jamspell\n",
            "    Running setup.py install for Jamspell ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-5e6wbifa/jamspell_c40d94abfe29458bab767d15a56d6724/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-5e6wbifa/jamspell_c40d94abfe29458bab767d15a56d6724/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-urno27yd/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/Jamspell Check the logs for full command output.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Symspellpy\n",
            "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 26.7 MB/s \n",
            "\u001b[?25hCollecting editdistpy>=0.1.3\n",
            "  Downloading editdistpy-0.1.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 61.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: editdistpy, Symspellpy\n",
            "Successfully installed Symspellpy-6.7.7 editdistpy-0.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from Textblob) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->Textblob) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->Textblob) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->Textblob) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->Textblob) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lexrank\n",
            "  Downloading lexrank-0.1.0-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from lexrank) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from lexrank) (1.21.6)\n",
            "Collecting urlextract>=0.7\n",
            "  Downloading urlextract-1.7.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: regex>=2017.11.9 in /usr/local/lib/python3.7/dist-packages (from lexrank) (2022.6.2)\n",
            "Collecting path.py>=10.5\n",
            "  Downloading path.py-12.5.0-py3-none-any.whl (2.3 kB)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from lexrank) (0.18.1)\n",
            "Collecting path\n",
            "  Downloading path-16.5.0-py3-none-any.whl (26 kB)\n",
            "Collecting uritools\n",
            "  Downloading uritools-4.0.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from urlextract>=0.7->lexrank) (2.10)\n",
            "Collecting platformdirs\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from urlextract>=0.7->lexrank) (3.8.0)\n",
            "Installing collected packages: uritools, platformdirs, path, urlextract, path.py, lexrank\n",
            "Successfully installed lexrank-0.1.0 path-16.5.0 path.py-12.5.0 platformdirs-2.5.2 uritools-4.0.0 urlextract-1.7.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3bfb74a5a956>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install gensim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install lexrank'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'punkt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'averaged_perceptron_tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c6c036",
      "metadata": {
        "id": "86c6c036"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import gensim\n",
        "import string\n",
        "import textacy\n",
        "import textstat\n",
        "import collections\n",
        "import language_tool_python\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "from path import Path\n",
        "from nltk import tokenize\n",
        "from scipy.stats import entropy\n",
        "from textacy import preprocessing\n",
        "from spellchecker import SpellChecker\n",
        "from lexrank import STOPWORDS, LexRank\n",
        "from textstat.textstat import textstatistics\n",
        "\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Import Data**"
      ],
      "metadata": {
        "id": "0pjidlGe49BE"
      },
      "id": "0pjidlGe49BE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fbfd375",
      "metadata": {
        "id": "6fbfd375"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('train.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Feature Engineering**"
      ],
      "metadata": {
        "id": "P_LcdFpG5Cko"
      },
      "id": "P_LcdFpG5Cko"
    },
    {
      "cell_type": "markdown",
      "id": "1eb8412c",
      "metadata": {
        "id": "1eb8412c"
      },
      "source": [
        "**1. Number of words without punctuations**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "123b8143",
      "metadata": {
        "id": "123b8143"
      },
      "outputs": [],
      "source": [
        "data['number_of_words'] = [len(text.split()) for text in data['full_text']]\n",
        "data['number_of_words'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d1362b3",
      "metadata": {
        "id": "1d1362b3"
      },
      "outputs": [],
      "source": [
        "data['words'] = [nltk.word_tokenize(x) for x in data['full_text']]\n",
        "data['words'].head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of words without punctuations\n",
        "data['alter_words'] = [nltk.RegexpTokenizer(r'\\w+').tokenize(x) for x in data['full_text']]\n",
        "data['alter_words'].head(10)"
      ],
      "metadata": {
        "id": "Ztkm63cOaYjc"
      },
      "id": "Ztkm63cOaYjc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "79b295f5",
      "metadata": {
        "id": "79b295f5"
      },
      "source": [
        "**2. Frequency of stop words**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb6c79a7",
      "metadata": {
        "id": "fb6c79a7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "data['stopwords'] = 0\n",
        "i = 0\n",
        "for w in data['words']:\n",
        "    counter = 0\n",
        "    for k in w:\n",
        "        if k in stopwords.words('english'):\n",
        "            counter += 1\n",
        "    data['stopwords'][i] = counter\n",
        "    i += 1\n",
        " \n",
        "\n",
        "data['stopwords'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4aeb05ee",
      "metadata": {
        "id": "4aeb05ee"
      },
      "outputs": [],
      "source": [
        "data['stopwords_frequency'] = data['stopwords'] / data['number_of_words']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f356d2",
      "metadata": {
        "id": "a8f356d2"
      },
      "source": [
        "**3. Average number of words in a sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08539e1b",
      "metadata": {
        "id": "08539e1b"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "data['sentence'] = 0\n",
        "data['sentence'] = [nltk.sent_tokenize(x) for x in data['full_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6e9aad",
      "metadata": {
        "id": "bd6e9aad"
      },
      "outputs": [],
      "source": [
        "data['sentence'] = [len(x) for x in data['sentence']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81342de",
      "metadata": {
        "id": "c81342de"
      },
      "outputs": [],
      "source": [
        "data['av_word_per_sen'] = data['number_of_words'] / data['sentence']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3af159",
      "metadata": {
        "id": "ec3af159"
      },
      "source": [
        "**4. Frequency of punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b717c89",
      "metadata": {
        "id": "2b717c89"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "i = 0\n",
        "data['punctuations'] = 0\n",
        "for word in data['words']:\n",
        "    count = len([c for c in word if c in list(string.punctuation)])\n",
        "    data['punctuations'][i] = count\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db183632",
      "metadata": {
        "id": "db183632"
      },
      "outputs": [],
      "source": [
        "data['word_punc'] = [len(x) for x in data['words']]\n",
        "data['punctuations'] = data['punctuations'] / data['word_punc']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99914291",
      "metadata": {
        "id": "99914291"
      },
      "source": [
        "**5. Automated Readability Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05cbdfc",
      "metadata": {
        "id": "e05cbdfc"
      },
      "outputs": [],
      "source": [
        "data['word_no_pun'] = [x.split() for x in data['full_text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "921eef75",
      "metadata": {
        "id": "921eef75"
      },
      "outputs": [],
      "source": [
        "data['ARI'] = 0\n",
        "i = 0\n",
        "for word_lst in data['word_no_pun']:\n",
        "    ttl_characters = 0\n",
        "    ari = 0\n",
        "    for word in word_lst:\n",
        "        ttl_characters += len(word)\n",
        "    ari = 4.71*(ttl_characters / data['number_of_words'][i]) + 0.5*(data['number_of_words'][i] / data['sentence'][i]) - 21.43\n",
        "    data['ARI'][i] = ari\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd132f1",
      "metadata": {
        "id": "0dd132f1"
      },
      "outputs": [],
      "source": [
        "data = data.drop(['stopwords','sentence','word_no_pun','word_punc'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Frequency of wrongly spelt words / text accuracy**"
      ],
      "metadata": {
        "id": "ueDjn8jtDYJp"
      },
      "id": "ueDjn8jtDYJp"
    },
    {
      "cell_type": "code",
      "source": [
        "spell = SpellChecker()\n",
        "data['freq_of_wrong_words'] = [len(spell.unknown(data['words'][i])) / len(data['full_text'][i].split()) for i in range(len(data))]\n",
        "data['freq_of_wrong_words'].head()"
      ],
      "metadata": {
        "id": "OptKftoYC-6G"
      },
      "id": "OptKftoYC-6G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. POS Tagging Related**"
      ],
      "metadata": {
        "id": "9GB33xKl-JqB"
      },
      "id": "9GB33xKl-JqB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply POS tagging\n",
        "# Stemming to reduce to base form before tagging\n",
        "tokenized_text=data['words'].tolist()\n",
        "num_of_verb=[]\n",
        "num_of_adj=[]\n",
        "num_of_adv=[]\n",
        "num_of_pron = []\n",
        "num_of_tran = []\n",
        "num_of_distinct_adj=[]\n",
        "num_of_distinct_adv=[]\n",
        "num_of_noun=[]\n",
        "for i in range(len(tokenized_text)):\n",
        "    sample = tokenized_text[i]\n",
        "    ps = PorterStemmer()\n",
        "    sample_tokens=[ps.stem(word) for word in sample]\n",
        "    sample_tokens\n",
        "    \n",
        "    # Set to be 1 for smoothing\n",
        "    sample_verb=1\n",
        "    sample_adj=1\n",
        "    sample_adv=1\n",
        "    sample_noun=1\n",
        "    sample_pron=1\n",
        "    sample_transition=1\n",
        "    \n",
        "    adj_list=[]\n",
        "    adv_list=[]\n",
        "\n",
        "    tag=nltk.pos_tag(sample_tokens)\n",
        "    for j in range(len(tag)):\n",
        "        if tag[j][1] == 'VB':\n",
        "            sample_verb+=1\n",
        "            continue\n",
        "        if tag[j][1][:2] == 'JJ':\n",
        "            sample_adj+=1\n",
        "            adj_list.append(tag[j][0])\n",
        "            continue\n",
        "        if tag[j][1][:2] == 'RB':\n",
        "            sample_adv+=1\n",
        "            adv_list.append(tag[j][0])\n",
        "            continue\n",
        "        if tag[j][1] == 'NN' or tag[j][1]=='NNS':\n",
        "            sample_noun+=1\n",
        "            continue\n",
        "        if tag[j][1][:3] == 'PRP':\n",
        "            sample_pron += 1\n",
        "        if tag[j][1] == 'CC':\n",
        "            sample_transition += 1\n",
        "        if j < len(tag) -2 and tag[j][1] == 'IN' and tag[j+1][1] == 'NN' and tag[j+2][1] == ',':\n",
        "            sample_transition += 1      \n",
        "\n",
        "    num_of_verb.append(sample_verb)\n",
        "    num_of_adj.append(sample_adj)\n",
        "    num_of_adv.append(sample_adv)\n",
        "    num_of_pron.append(sample_pron)\n",
        "    num_of_tran.append(sample_transition)\n",
        "    num_of_noun.append(sample_noun)\n",
        "    num_of_distinct_adj.append(len(np.unique(np.array(adj_list))))\n",
        "    num_of_distinct_adv.append(len(np.unique(np.array(adv_list)))) "
      ],
      "metadata": {
        "id": "Eu8rE8obuNiO"
      },
      "id": "Eu8rE8obuNiO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['freq_of_verb']=num_of_verb/data['number_of_words']\n",
        "data['freq_of_adj']=num_of_adj/data['number_of_words']\n",
        "data['freq_of_adv']=num_of_adv/data['number_of_words']\n",
        "data['freq_of_distinct_adj']=num_of_distinct_adj/data['number_of_words']\n",
        "data['freq_of_distinct_adv']=num_of_distinct_adv/data['number_of_words']\n",
        "data['freq_of_noun']=num_of_noun/data['number_of_words']\n",
        "data['freq_of_transition']=num_of_tran/data['number_of_words']\n",
        "data['freq_of_pronoun']=num_of_pron/data['number_of_words']\n",
        "data.head()"
      ],
      "metadata": {
        "id": "80aBDdY2uRBv"
      },
      "id": "80aBDdY2uRBv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross feature\n",
        "data['noun_to_adj']=data['freq_of_adj']/data['freq_of_noun']\n",
        "data['verb_to_adv']=data['freq_of_adv']/data['freq_of_verb']\n",
        "\n",
        "data['phrase_diversity']=data['noun_to_adj']*data['verb_to_adv']\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LVOIoU12uVZT"
      },
      "id": "LVOIoU12uVZT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into sentences\n",
        "sentence_complexity=[]\n",
        "for i in range(3911):\n",
        "    sample=data['corrected_text'][i]\n",
        "    sentences=nltk.sent_tokenize(sample)\n",
        "    entropy_list=[]\n",
        "    # For each sentence, find total number of each tag\n",
        "    # Calculate entropy\n",
        "    # Average all entropy to get avg sentence complexity for this text\n",
        "    for sentence in sentences:\n",
        "        ps = PorterStemmer()\n",
        "        stemmed=[ps.stem(word) for word in sentence]\n",
        "        tags=nltk.pos_tag(stemmed)\n",
        "\n",
        "        tag_dic={}\n",
        "        for tag in tags:\n",
        "            if tag[1] in tag_dic.keys():\n",
        "                tag_dic[tag[1]]+=1\n",
        "            else:\n",
        "                tag_dic[tag[1]]=1\n",
        "\n",
        "        entropy_list.append(entropy(list(tag_dic.values())))\n",
        "    \n",
        "    sentence_complexity.append(sum(entropy_list) / len(entropy_list))\n",
        "\n",
        "data['sentence_complexity']=sentence_complexity"
      ],
      "metadata": {
        "id": "HFIG8aIP4jwW"
      },
      "id": "HFIG8aIP4jwW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Sentiment Scores**"
      ],
      "metadata": {
        "id": "MGmR99B8_ZS7"
      },
      "id": "MGmR99B8_ZS7"
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')\n",
        "  \n",
        "sia=SentimentIntensityAnalyzer()\n",
        "\n",
        "data['sentiment_compound']=data['full_text'].apply(lambda x: sia.polarity_scores(x).get('compound'))\n",
        "data['sentiment_positive']=data['full_text'].apply(lambda x: sia.polarity_scores(x).get('pos'))\n",
        "data['sentiment_negative']=data['full_text'].apply(lambda x: sia.polarity_scores(x).get('neg'))"
      ],
      "metadata": {
        "id": "JElcMzU8_eyu"
      },
      "id": "JElcMzU8_eyu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Grammar and Spell Checking**"
      ],
      "metadata": {
        "id": "jyCrFtN9_0U1"
      },
      "id": "jyCrFtN9_0U1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of grammar errors and replace with correct form\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "data['num_of_grammar_errors'] = data['full_text'].apply(lambda x: len(tool.check(x.replace('\\n',''))))\n",
        "data['corrected_text'] = data['full_text'].apply(lambda x: tool.correct(x.replace('\\n','')))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "_49j8jsw_7Fo"
      },
      "id": "_49j8jsw_7Fo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short Forms (combined with grammar error)"
      ],
      "metadata": {
        "id": "574hueEmAMpr"
      },
      "id": "574hueEmAMpr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Find short forms\n",
        "collections = {'u':'you', 'b':'B'}\n",
        "\n",
        "def num_of_short_form(text):\n",
        "    num_of_error=0\n",
        "    splitted_text=text.split()\n",
        "    for word in splitted_text:\n",
        "        if word in collections.keys():\n",
        "            num_of_error += 1\n",
        "    return num_of_error     "
      ],
      "metadata": {
        "id": "D9uPZrnxAUcM"
      },
      "id": "D9uPZrnxAUcM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['num_of_short_forms'] = data['corrected_text'].apply(lambda x: num_of_short_form(x))\n",
        "data['Incorrect_form_ratio'] = (data['num_of_grammar_errors'] + data['num_of_short_forms'])/ data['number_of_words']\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ArLs6D0MAXR4"
      },
      "id": "ArLs6D0MAXR4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11. Scores from textstat**"
      ],
      "metadata": {
        "id": "S2LuDSM8Aeea"
      },
      "id": "S2LuDSM8Aeea"
    },
    {
      "cell_type": "code",
      "source": [
        "# Flesch reading ease: score between 1 to 100, 100 means highest readability\n",
        "data['flesch_reading_ease']=data['full_text'].apply(lambda x: textstat.flesch_reading_ease(preprocessing.normalize.whitespace(x)))\n",
        "\n",
        "# Flesch kincaid grade: required education level in order to read the text\n",
        "data['flesch_kincaid_grade']=data['full_text'].apply(lambda x: textstat.flesch_kincaid_grade(preprocessing.normalize.whitespace(x)))\n",
        "\n",
        "# Dale-Chall readability score: higher score means harder to understand\n",
        "data['dale_chall_readability_score']=data['full_text'].apply(lambda x: textstat.dale_chall_readability_score(preprocessing.normalize.whitespace(x)))\n",
        "\n",
        "# Text_standard score: a score provided by the library based on the score of other measures\n",
        "data['text_standard']=data['full_text'].apply(lambda x: textstat.text_standard(preprocessing.normalize.whitespace(x), float_output=True))\n",
        "\n",
        "# McAlpine-EFLAW score: higher the score, higher to understand, should be aiming below 25\n",
        "data['mcalpine_eflaw']=data['full_text'].apply(lambda x: 25 - (textstat.mcalpine_eflaw(preprocessing.normalize.whitespace(x))))"
      ],
      "metadata": {
        "id": "Q0Xcb2HoAhtf"
      },
      "id": "Q0Xcb2HoAhtf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "ClptwgZXAwxr"
      },
      "id": "ClptwgZXAwxr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12. Difficult Word Extraction**"
      ],
      "metadata": {
        "id": "EBKTEO-KAymn"
      },
      "id": "EBKTEO-KAymn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes VERY LONG\n",
        "# Difficult words are those with syllables >= 2 (can customize the setting)\n",
        "\n",
        "def syllables_count(word):\n",
        "    return textstatistics().syllable_count(word)\n",
        "\n",
        "def break_sentences(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    return list(doc.sents)\n",
        "\n",
        "def difficult_words(text):  \n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    # Find all words in the text\n",
        "    words = []\n",
        "    sentences = break_sentences(text)\n",
        "    for sentence in sentences:\n",
        "        words += [str(token) for token in sentence]\n",
        " \n",
        "    diff_words_set = set()\n",
        "     \n",
        "    for word in words:\n",
        "        syllable_count = syllables_count(word)\n",
        "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
        "            diff_words_set.add(word)\n",
        " \n",
        "    return len(diff_words_set)"
      ],
      "metadata": {
        "id": "NjrIfCNdA1us"
      },
      "id": "NjrIfCNdA1us",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['number_of_diff_words'] = data['full_text'].apply(lambda x: difficult_words(x))\n",
        "data['freq_diff_words']=data['number_of_diff_words'] / data['number_of_words']"
      ],
      "metadata": {
        "id": "VIX7ZPFJA9Kg"
      },
      "id": "VIX7ZPFJA9Kg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Type-Token Ratio (TTR)**"
      ],
      "metadata": {
        "id": "YvA8W2qcBBz0"
      },
      "id": "YvA8W2qcBBz0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Type-token ratio for lexical measure\n",
        "# Larger means higher lexical variation\n",
        "def ttr(sample):\n",
        "    sample_words=sample.split()\n",
        "    n_words = len(sample_words)\n",
        "\n",
        "    # remove all punctuations\n",
        "    for i in range(n_words):\n",
        "        for c in string.punctuation:\n",
        "            sample_words[i] = sample_words[i].replace(c,'')\n",
        "\n",
        "    # remove empty words\n",
        "    sample_words = list(filter(None, sample_words))\n",
        "    n_words = len(sample)\n",
        "\n",
        "    # count each word\n",
        "    word_count = collections.Counter(sample_words)\n",
        "\n",
        "    # get the sorted list of unique words\n",
        "    unique_words = list(word_count.keys())\n",
        "    unique_words.sort()\n",
        "\n",
        "    n_unique = len(unique_words)\n",
        "    ttr = len(word_count)/float(n_words)\n",
        "    return ttr"
      ],
      "metadata": {
        "id": "lg_3kOV2BFcR"
      },
      "id": "lg_3kOV2BFcR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['ttr']=data['corrected_text'].apply(lambda x: ttr(x))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "ToSr8JybBYpI"
      },
      "id": "ToSr8JybBYpI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**14. Coherence Score**"
      ],
      "metadata": {
        "id": "vkUhl9_twHHM"
      },
      "id": "vkUhl9_twHHM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentences to words\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "nRMMHcnGwKfm"
      },
      "id": "nRMMHcnGwKfm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for final calculation\n",
        "def coherence_score(essay):\n",
        "    # Text to sentence then to words\n",
        "    sentences=nltk.tokenize.sent_tokenize(essay)\n",
        "    sentences_df=pd.DataFrame(sentences, columns=['sentence'])\n",
        "    \n",
        "    # Remove punctuation\n",
        "    sentences_df['sentence'] = sentences_df['sentence'].apply(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "    # Convert the titles to lowercase\n",
        "    sentences_df['sentence'] = sentences_df['sentence'].apply(lambda x: x.lower())\n",
        "    \n",
        "    sentences = sentences_df.sentence.values.tolist()\n",
        "    words = list(sent_to_words(sentences))\n",
        "    \n",
        "    # Build bigram model\n",
        "    bigram = gensim.models.Phrases(words, min_count=5, threshold=100)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    \n",
        "    # Remove Stop Words\n",
        "    data_words_nostops = remove_stopwords(words)\n",
        "    # Form Bigrams\n",
        "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "    # Do lemmatization keeping only noun, adj, vb, adv\n",
        "    data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "    \n",
        "    # Create Dictionary\n",
        "    id2word = corpora.Dictionary(data_lemmatized)\n",
        "    # Create Corpus\n",
        "    texts = data_lemmatized\n",
        "    # Term Document Frequency\n",
        "    corpus = [id2word.doc2bow(text) for text in texts]\n",
        "    \n",
        "    # Build LDA model\n",
        "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                        id2word=id2word,\n",
        "                        num_topics=5, \n",
        "                        random_state=100,\n",
        "                        chunksize=100,\n",
        "                        passes=10,\n",
        "                        per_word_topics=True)\n",
        "    \n",
        "    # Compute coherence score\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "    coherence_lda = coherence_model_lda.get_coherence()\n",
        "    \n",
        "    return coherence_lda\n",
        "\n",
        "data['coherence_score']=data['corrected_text'].apply(lambda x: coherence_score(x))"
      ],
      "metadata": {
        "id": "2YeG-Zo5waBe"
      },
      "id": "2YeG-Zo5waBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15. Lexrank**"
      ],
      "metadata": {
        "id": "kavLTLfKBiP4"
      },
      "id": "kavLTLfKBiP4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrected text to documents for lexrank, no need to run again\n",
        "sample_df=pd.DataFrame(data['corrected_text'])\n",
        "for index, row in sample_df.iterrows():\n",
        "    if index > len(sample_df):\n",
        "       break\n",
        "    else:\n",
        "       f = open('Text Documents/'+str(index)+'.txt', 'w', encoding='utf-8')\n",
        "       f.write(row[0])\n",
        "       f.close()\n",
        "       index+=1"
      ],
      "metadata": {
        "id": "G7jDWtoQBlAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "5768b800-6853-4600-e42d-9a36377dabb6"
      },
      "id": "G7jDWtoQBlAO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-c18aed4a5b35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m        \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Text Documents/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m        \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m        \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Text Documents/0.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "documents_dir = Path('Text Documents')\n",
        "\n",
        "for file_path in documents_dir.files('*.txt'):\n",
        "    with file_path.open(mode='rt', encoding='utf-8') as fp:\n",
        "        documents.append(fp.readlines())\n",
        "        \n",
        "lxr = LexRank(documents, stopwords=STOPWORDS['en'])"
      ],
      "metadata": {
        "id": "zZGS4L5wB0Xr"
      },
      "id": "zZGS4L5wB0Xr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['lexrank_avg_min_diff'] = data['corrected_text'].apply(lambda x: sum(lxr.rank_sentences(tokenize.sent_tokenize(x), threshold=None))/len(lxr.rank_sentences(tokenize.sent_tokenize(x),threshold=None))-min(lxr.rank_sentences(tokenize.sent_tokenize(x), threshold=None)))\n",
        "data['lexrank_interquartile'] = data['corrected_text'].apply(lambda x: np.percentile(lxr.rank_sentences(tokenize.sent_tokenize(x), threshold=None),[25,75])[1]-np.percentile(lxr.rank_sentences(tokenize.sent_tokenize(x), threshold=None),[25,75])[0])"
      ],
      "metadata": {
        "id": "O6yguX1uCOHm"
      },
      "id": "O6yguX1uCOHm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}