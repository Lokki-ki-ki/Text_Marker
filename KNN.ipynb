{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(predictions):\n",
    "    predictions = predictions.tolist()\n",
    "    result_list=[]\n",
    "    for pred in predictions:\n",
    "        result = pred // 0.5 * 0.5\n",
    "        if (pred - result) > 0.25:\n",
    "            result += 0.5\n",
    "        if result < 1.0:\n",
    "            result = 1.0\n",
    "        if result > 5.0:\n",
    "            result = 5.0\n",
    "        result_list.append(result)\n",
    "    return result_list\n",
    "\n",
    "# Accuracy score\n",
    "def accuracy(Ypred, Ytrue):\n",
    "    Ytrue = Ytrue.tolist()\n",
    "    accurate = 0\n",
    "    for i in range(len(Ytrue)):\n",
    "        if Ytrue[i] == Ypred[i]:\n",
    "            accurate += 1\n",
    "    return accurate / len(Ytrue)\n",
    "\n",
    "def accuracy_range(Ytrue, Ypred):\n",
    "    Ytrue=Ytrue.tolist()\n",
    "    accurate_range=0\n",
    "    for i in range(len(Ytrue)):\n",
    "        if abs(Ytrue[i] - Ypred[i])<=0.5:\n",
    "            accurate_range+=1\n",
    "    return accurate_range/len(Ytrue)\n",
    "\n",
    "# Total error / total number of points => by average what's the error for each point\n",
    "def error_rate(Ytrue, Ypred):\n",
    "    Ytrue=Ytrue.tolist()\n",
    "    error=0\n",
    "    for i in range(len(Ytrue)):\n",
    "        error += abs(Ytrue[i] - Ypred[i])\n",
    "    return error/len(Ytrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use 35 features to predict cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/lokki/Documents/GitHub/Text_Marker/Processed_Data.csv\")\n",
    "df = df.dropna(axis=0)\n",
    "# y = df[[\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]]\n",
    "y_coh = df[\"cohesion\"]\n",
    "X = df.iloc[:, 7:]\n",
    "X = X.drop(\"corrected_text\", axis=1)\n",
    "X[\"text_standard\"].mask(df[\"text_standard\"] == \"-\", 0, inplace=True)\n",
    "X[\"verb_to_adv\"].mask(np.isinf(df[\"verb_to_adv\"]), 0, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_coh, test_size=1/4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2842 candidates, totalling 14210 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={'leaf_size': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n",
       "                         'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                         13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                         23, 24, 25, 26, 27, 28, 29],\n",
       "                         'p': [1, 2]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "## Hyperparameters tuning\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "knn = GridSearchCV(KNeighborsRegressor(), hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(leaf_size=1, n_neighbors=28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy range: 0.6094069529652352 \n",
      " accuracy: 0.3343558282208589 \n",
      " error rate: 0.4437627811860941 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = knn.predict(X_test)\n",
    "adj_pred = result(pred)\n",
    "\n",
    "print(\"accuracy range: %s \\n accuracy: %s \\n error rate: %s \\n\" \n",
    "    % (accuracy_range(y_test, pred), accuracy(adj_pred, y_test), error_rate(y_test, adj_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.82142857, 2.76785714, 3.5       , 3.21428571, 2.89285714,\n",
       "       3.26785714, 2.66071429, 2.625     , 2.89285714, 3.01785714,\n",
       "       3.07142857, 3.19642857, 3.30357143, 3.32142857, 3.03571429,\n",
       "       3.55357143, 3.69642857, 3.71428571, 3.76785714, 3.32142857,\n",
       "       3.42857143, 3.625     , 3.05357143, 3.17857143, 3.19642857,\n",
       "       2.82142857, 3.17857143, 3.25      , 2.80357143, 3.33928571,\n",
       "       3.66071429, 3.        , 2.71428571, 3.21428571, 3.30357143,\n",
       "       3.19642857, 2.875     , 3.10714286, 3.5       , 2.83928571,\n",
       "       2.91071429, 3.19642857, 2.89285714, 2.89285714, 3.625     ,\n",
       "       3.39285714, 3.01785714, 3.        , 3.42857143, 3.60714286,\n",
       "       3.67857143, 3.17857143, 3.28571429, 2.67857143, 3.19642857,\n",
       "       3.14285714, 3.80357143, 2.66071429, 2.92857143, 2.67857143,\n",
       "       3.08928571, 2.875     , 3.5       , 3.58928571, 2.85714286,\n",
       "       3.60714286, 2.91071429, 3.33928571, 3.48214286, 3.625     ,\n",
       "       2.85714286, 3.53571429, 3.26785714, 3.03571429, 3.32142857,\n",
       "       3.10714286, 3.42857143, 3.42857143, 2.94642857, 2.80357143,\n",
       "       2.82142857, 2.85714286, 3.21428571, 3.01785714, 3.57142857,\n",
       "       2.94642857, 2.76785714, 3.44642857, 3.69642857, 3.03571429,\n",
       "       3.35714286, 3.30357143, 3.28571429, 2.85714286, 3.51785714,\n",
       "       3.55357143, 3.        , 3.125     , 3.39285714, 3.53571429,\n",
       "       2.80357143, 2.875     , 3.73214286, 2.80357143, 3.03571429,\n",
       "       2.82142857, 3.03571429, 2.91071429, 3.03571429, 3.05357143,\n",
       "       3.39285714, 3.64285714, 3.32142857, 3.5       , 3.01785714,\n",
       "       2.96428571, 3.71428571, 3.25      , 2.58928571, 3.23214286,\n",
       "       2.78571429, 2.98214286, 3.32142857, 3.03571429, 2.46428571,\n",
       "       2.875     , 3.05357143, 3.26785714, 3.25      , 2.48214286,\n",
       "       3.05357143, 3.01785714, 2.82142857, 3.03571429, 2.875     ,\n",
       "       3.01785714, 2.78571429, 3.05357143, 3.51785714, 3.58928571,\n",
       "       3.33928571, 2.83928571, 3.53571429, 3.28571429, 2.94642857,\n",
       "       2.85714286, 2.82142857, 3.21428571, 3.26785714, 2.85714286,\n",
       "       2.85714286, 2.83928571, 3.19642857, 2.96428571, 2.83928571,\n",
       "       3.17857143, 3.03571429, 3.46428571, 3.07142857, 3.30357143,\n",
       "       3.03571429, 3.25      , 2.98214286, 2.98214286, 2.98214286,\n",
       "       3.41071429, 3.26785714, 3.75      , 3.14285714, 3.26785714,\n",
       "       3.48214286, 3.46428571, 3.80357143, 2.57142857, 2.89285714,\n",
       "       3.58928571, 3.14285714, 2.92857143, 3.17857143, 3.51785714,\n",
       "       2.82142857, 2.64285714, 2.82142857, 2.64285714, 3.19642857,\n",
       "       3.55357143, 3.41071429, 3.53571429, 3.64285714, 3.19642857,\n",
       "       2.5       , 3.14285714, 3.23214286, 3.28571429, 3.33928571,\n",
       "       3.48214286, 2.91071429, 3.05357143, 3.33928571, 3.67857143,\n",
       "       2.89285714, 3.25      , 3.14285714, 2.82142857, 3.07142857,\n",
       "       2.80357143, 3.        , 3.10714286, 3.        , 2.75      ,\n",
       "       2.76785714, 3.32142857, 3.14285714, 2.76785714, 3.01785714,\n",
       "       3.48214286, 3.08928571, 2.73214286, 3.28571429, 3.42857143,\n",
       "       3.48214286, 3.5       , 2.64285714, 3.23214286, 3.64285714,\n",
       "       2.94642857, 2.94642857, 3.44642857, 3.28571429, 3.33928571,\n",
       "       2.875     , 2.60714286, 2.85714286, 3.03571429, 3.23214286,\n",
       "       3.07142857, 3.32142857, 2.91071429, 2.96428571, 3.5       ,\n",
       "       3.05357143, 3.23214286, 3.57142857, 2.67857143, 2.75      ,\n",
       "       2.83928571, 3.48214286, 3.16071429, 2.83928571, 2.23214286,\n",
       "       2.89285714, 3.625     , 3.44642857, 3.25      , 3.46428571,\n",
       "       2.64285714, 3.51785714, 3.07142857, 2.83928571, 2.78571429,\n",
       "       3.80357143, 2.64285714, 3.48214286, 2.89285714, 3.44642857,\n",
       "       3.42857143, 3.28571429, 2.25      , 3.17857143, 3.32142857,\n",
       "       3.33928571, 3.19642857, 3.03571429, 3.19642857, 3.05357143,\n",
       "       3.83928571, 2.78571429, 3.33928571, 3.41071429, 3.625     ,\n",
       "       2.96428571, 2.78571429, 2.83928571, 3.26785714, 2.89285714,\n",
       "       3.30357143, 2.94642857, 3.28571429, 2.96428571, 2.80357143,\n",
       "       3.375     , 3.26785714, 2.75      , 2.82142857, 3.375     ,\n",
       "       3.14285714, 2.75      , 2.73214286, 3.23214286, 3.55357143,\n",
       "       3.35714286, 3.375     , 3.66071429, 3.10714286, 2.69642857,\n",
       "       3.08928571, 3.60714286, 2.44642857, 3.42857143, 3.67857143,\n",
       "       3.07142857, 3.07142857, 3.        , 3.48214286, 2.78571429,\n",
       "       3.26785714, 3.30357143, 2.80357143, 3.375     , 3.05357143,\n",
       "       3.16071429, 2.69642857, 3.07142857, 2.78571429, 2.85714286,\n",
       "       3.44642857, 2.89285714, 2.92857143, 2.89285714, 3.75      ,\n",
       "       3.53571429, 2.67857143, 3.57142857, 2.875     , 3.41071429,\n",
       "       2.85714286, 2.98214286, 3.60714286, 3.30357143, 2.875     ,\n",
       "       2.92857143, 3.53571429, 2.73214286, 3.58928571, 2.66071429,\n",
       "       3.33928571, 2.82142857, 3.05357143, 2.78571429, 2.82142857,\n",
       "       3.32142857, 2.98214286, 2.92857143, 3.42857143, 3.125     ,\n",
       "       2.67857143, 2.92857143, 2.67857143, 2.89285714, 3.03571429,\n",
       "       3.46428571, 3.51785714, 3.01785714, 2.96428571, 3.76785714,\n",
       "       2.64285714, 2.82142857, 3.5       , 3.05357143, 3.        ,\n",
       "       3.32142857, 3.03571429, 2.83928571, 3.57142857, 3.53571429,\n",
       "       3.30357143, 3.26785714, 3.23214286, 3.01785714, 2.78571429,\n",
       "       3.05357143, 2.64285714, 2.66071429, 3.32142857, 3.01785714,\n",
       "       3.75      , 3.41071429, 3.14285714, 3.66071429, 2.85714286,\n",
       "       3.42857143, 3.23214286, 3.26785714, 2.66071429, 3.17857143,\n",
       "       2.67857143, 3.67857143, 3.10714286, 3.39285714, 2.94642857,\n",
       "       2.89285714, 3.46428571, 2.23214286, 3.39285714, 3.28571429,\n",
       "       3.10714286, 3.375     , 3.58928571, 3.25      , 2.94642857,\n",
       "       2.76785714, 3.51785714, 2.66071429, 3.71428571, 3.19642857,\n",
       "       2.73214286, 3.05357143, 2.64285714, 3.48214286, 3.30357143,\n",
       "       3.75      , 3.64285714, 3.46428571, 3.17857143, 2.96428571,\n",
       "       2.92857143, 3.14285714, 2.89285714, 2.96428571, 3.48214286,\n",
       "       2.94642857, 3.41071429, 3.03571429, 3.05357143, 3.01785714,\n",
       "       3.35714286, 3.19642857, 2.73214286, 3.41071429, 2.67857143,\n",
       "       3.01785714, 3.53571429, 3.48214286, 3.23214286, 3.5       ,\n",
       "       3.73214286, 3.32142857, 3.21428571, 2.89285714, 3.69642857,\n",
       "       3.28571429, 3.01785714, 3.44642857, 3.28571429, 3.41071429,\n",
       "       3.44642857, 3.64285714, 3.375     , 3.17857143, 2.71428571,\n",
       "       2.58928571, 3.5       , 3.32142857, 3.33928571, 2.83928571,\n",
       "       3.58928571, 3.125     , 2.80357143, 3.66071429, 2.96428571,\n",
       "       3.125     , 3.01785714, 3.125     , 3.75      , 3.66071429,\n",
       "       3.01785714, 3.08928571, 3.5       , 3.25      , 2.76785714,\n",
       "       3.125     , 3.03571429, 2.42857143, 3.32142857, 2.75      ,\n",
       "       3.60714286, 3.32142857, 2.67857143, 3.19642857, 3.57142857,\n",
       "       2.60714286, 2.875     , 3.08928571, 3.19642857, 3.51785714,\n",
       "       3.01785714, 2.83928571, 2.48214286, 3.23214286, 3.55357143,\n",
       "       3.03571429, 2.33928571, 3.69642857, 2.75      , 3.35714286,\n",
       "       3.30357143, 3.42857143, 3.32142857, 3.33928571, 3.30357143,\n",
       "       2.71428571, 3.01785714, 3.14285714, 3.60714286, 3.125     ,\n",
       "       3.58928571, 3.19642857, 3.375     , 3.08928571, 3.48214286,\n",
       "       2.85714286, 3.07142857, 3.60714286, 3.21428571, 3.21428571,\n",
       "       3.125     , 3.28571429, 2.75      , 2.96428571, 2.71428571,\n",
       "       3.28571429, 3.21428571, 2.91071429, 3.25      , 3.03571429,\n",
       "       3.44642857, 2.75      , 3.16071429, 3.125     , 3.32142857,\n",
       "       2.83928571, 2.91071429, 3.03571429, 2.75      , 3.44642857,\n",
       "       3.01785714, 3.26785714, 3.375     , 3.76785714, 2.82142857,\n",
       "       3.21428571, 2.35714286, 3.67857143, 3.125     , 2.85714286,\n",
       "       3.19642857, 3.60714286, 2.80357143, 2.82142857, 2.82142857,\n",
       "       3.03571429, 3.25      , 2.75      , 3.26785714, 3.375     ,\n",
       "       3.66071429, 3.75      , 2.76785714, 2.82142857, 3.35714286,\n",
       "       3.46428571, 3.39285714, 3.19642857, 3.30357143, 3.41071429,\n",
       "       2.85714286, 3.64285714, 2.66071429, 3.5       , 3.35714286,\n",
       "       3.25      , 3.26785714, 3.01785714, 3.32142857, 3.14285714,\n",
       "       3.03571429, 3.48214286, 3.33928571, 3.        , 3.25      ,\n",
       "       3.64285714, 3.32142857, 3.23214286, 2.92857143, 2.67857143,\n",
       "       3.25      , 3.26785714, 3.26785714, 2.875     , 3.66071429,\n",
       "       3.46428571, 3.73214286, 3.21428571, 3.25      , 3.33928571,\n",
       "       2.78571429, 3.28571429, 2.92857143, 2.82142857, 3.        ,\n",
       "       3.05357143, 2.71428571, 3.5       , 3.41071429, 3.19642857,\n",
       "       3.17857143, 3.41071429, 2.96428571, 2.83928571, 2.92857143,\n",
       "       3.21428571, 3.14285714, 2.89285714, 3.30357143, 3.44642857,\n",
       "       3.10714286, 3.26785714, 3.48214286, 3.21428571, 3.71428571,\n",
       "       3.48214286, 3.35714286, 3.53571429, 3.10714286, 3.28571429,\n",
       "       3.375     , 3.19642857, 2.80357143, 2.76785714, 3.125     ,\n",
       "       3.25      , 3.125     , 3.625     , 3.58928571, 3.07142857,\n",
       "       3.82142857, 3.25      , 3.58928571, 3.35714286, 2.66071429,\n",
       "       2.875     , 3.53571429, 2.98214286, 2.73214286, 3.        ,\n",
       "       3.28571429, 2.76785714, 3.51785714, 3.5       , 3.        ,\n",
       "       2.76785714, 2.85714286, 3.57142857, 3.32142857, 3.60714286,\n",
       "       3.03571429, 2.91071429, 2.73214286, 3.07142857, 2.85714286,\n",
       "       2.78571429, 3.58928571, 3.125     , 3.17857143, 2.98214286,\n",
       "       3.82142857, 2.94642857, 3.33928571, 2.80357143, 3.07142857,\n",
       "       3.19642857, 3.33928571, 3.        , 2.75      , 2.83928571,\n",
       "       3.41071429, 3.03571429, 2.91071429, 3.07142857, 3.83928571,\n",
       "       2.73214286, 3.01785714, 2.92857143, 3.51785714, 3.25      ,\n",
       "       2.875     , 3.35714286, 3.30357143, 2.89285714, 3.5       ,\n",
       "       2.82142857, 3.21428571, 3.58928571, 2.94642857, 2.60714286,\n",
       "       2.76785714, 3.25      , 2.57142857, 3.25      , 3.03571429,\n",
       "       3.125     , 3.25      , 2.85714286, 3.39285714, 3.05357143,\n",
       "       2.89285714, 3.30357143, 3.23214286, 2.83928571, 2.91071429,\n",
       "       2.625     , 3.67857143, 3.39285714, 3.125     , 3.26785714,\n",
       "       2.5       , 3.76785714, 2.875     , 3.30357143, 3.14285714,\n",
       "       3.375     , 3.16071429, 3.75      , 3.33928571, 2.92857143,\n",
       "       3.39285714, 3.44642857, 2.875     , 2.96428571, 3.55357143,\n",
       "       3.03571429, 2.73214286, 3.51785714, 3.57142857, 2.75      ,\n",
       "       3.32142857, 3.55357143, 3.44642857, 3.48214286, 3.33928571,\n",
       "       3.03571429, 2.82142857, 2.76785714, 3.33928571, 2.73214286,\n",
       "       2.91071429, 3.21428571, 2.83928571, 2.73214286, 3.05357143,\n",
       "       2.85714286, 3.08928571, 2.73214286, 3.        , 3.10714286,\n",
       "       2.91071429, 3.08928571, 3.32142857, 3.21428571, 2.89285714,\n",
       "       3.03571429, 3.41071429, 3.32142857, 2.96428571, 3.        ,\n",
       "       3.53571429, 2.75      , 3.55357143, 2.85714286, 2.89285714,\n",
       "       3.46428571, 3.44642857, 3.17857143, 2.85714286, 3.25      ,\n",
       "       2.96428571, 2.64285714, 2.98214286, 2.78571429, 2.94642857,\n",
       "       3.        , 3.08928571, 2.96428571, 2.64285714, 3.19642857,\n",
       "       3.39285714, 2.98214286, 3.01785714, 3.14285714, 2.94642857,\n",
       "       3.21428571, 3.5       , 3.46428571, 2.80357143, 2.69642857,\n",
       "       2.41071429, 3.05357143, 2.89285714, 2.78571429, 3.44642857,\n",
       "       3.19642857, 3.32142857, 3.28571429, 2.80357143, 2.91071429,\n",
       "       3.67857143, 3.5       , 3.10714286, 3.53571429, 3.5       ,\n",
       "       2.85714286, 3.30357143, 3.55357143, 2.66071429, 3.30357143,\n",
       "       3.44642857, 3.53571429, 3.07142857, 3.30357143, 3.35714286,\n",
       "       2.75      , 3.14285714, 3.48214286, 3.07142857, 3.25      ,\n",
       "       3.03571429, 3.23214286, 3.39285714, 3.41071429, 2.85714286,\n",
       "       3.125     , 3.76785714, 2.91071429, 2.89285714, 3.01785714,\n",
       "       2.73214286, 2.98214286, 3.14285714, 3.48214286, 3.05357143,\n",
       "       3.33928571, 3.33928571, 3.08928571, 2.25      , 3.69642857,\n",
       "       3.39285714, 3.32142857, 2.78571429, 3.32142857, 3.5       ,\n",
       "       3.55357143, 3.64285714, 3.125     , 3.23214286, 3.23214286,\n",
       "       3.        , 3.        , 2.66071429, 3.44642857, 2.875     ,\n",
       "       2.73214286, 2.83928571, 3.32142857, 3.48214286, 2.41071429,\n",
       "       3.46428571, 2.73214286, 3.46428571, 3.78571429, 3.64285714,\n",
       "       3.73214286, 3.58928571, 2.92857143, 3.75      , 2.55357143,\n",
       "       2.73214286, 3.57142857, 3.33928571, 3.        , 2.83928571,\n",
       "       3.41071429, 3.        , 2.78571429, 2.89285714, 3.26785714,\n",
       "       2.89285714, 3.80357143, 3.125     , 3.07142857, 3.07142857,\n",
       "       2.78571429, 3.14285714, 3.125     , 3.26785714, 2.41071429,\n",
       "       3.42857143, 3.16071429, 2.75      , 2.94642857, 3.39285714,\n",
       "       3.125     , 3.60714286, 2.66071429, 3.32142857, 3.21428571,\n",
       "       3.03571429, 2.94642857, 3.01785714, 2.92857143, 3.39285714,\n",
       "       2.76785714, 2.85714286, 2.875     , 2.98214286, 2.96428571,\n",
       "       3.30357143, 2.76785714, 2.71428571, 2.21428571, 3.26785714,\n",
       "       2.98214286, 3.        , 3.375     , 3.17857143, 3.32142857,\n",
       "       2.92857143, 2.98214286, 3.01785714, 2.80357143, 3.33928571,\n",
       "       3.57142857, 3.69642857, 3.48214286, 3.10714286, 2.82142857,\n",
       "       3.66071429, 2.92857143, 3.60714286, 2.89285714, 3.53571429,\n",
       "       2.51785714, 3.32142857, 3.78571429, 3.33928571, 3.375     ,\n",
       "       3.66071429, 3.33928571, 3.        , 2.67857143, 2.83928571,\n",
       "       2.92857143, 3.26785714, 3.58928571, 3.48214286, 2.41071429,\n",
       "       3.17857143, 2.91071429, 3.17857143, 2.78571429, 2.85714286,\n",
       "       3.125     , 3.46428571, 2.76785714, 3.69642857, 3.44642857,\n",
       "       3.35714286, 3.60714286, 3.16071429])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the vocabulary first then use it as feature to predict cohesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vocab = df[\"vocabulary\"]\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y_vocab, test_size=1/4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2842 candidates, totalling 14210 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={'leaf_size': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n",
       "                         'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                         13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                         23, 24, 25, 26, 27, 28, 29],\n",
       "                         'p': [1, 2]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_vocab = GridSearchCV(KNeighborsRegressor(), hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
    "knn_vocab.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "pickle.dump(knn_vocab, open('knn_vocab.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature = knn_vocab.predict(X)\n",
    "vocab = pd.Series(new_feature)\n",
    "features = X.loc[:]\n",
    "features[\"vocab\"] = vocab\n",
    "target = df[\"cohesion\"]\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(features, target, test_size=1/4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2842 candidates, totalling 14210 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={'leaf_size': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                       13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                       23, 24, 25, 26, 27, 28, 29, 30, ...],\n",
       "                         'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                         13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                         23, 24, 25, 26, 27, 28, 29],\n",
       "                         'p': [1, 2]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_coh = GridSearchCV(KNeighborsRegressor(), hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
    "knn_coh.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(leaf_size=1, n_neighbors=28)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_coh.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy range: 0.6094069529652352 \n",
      " accuracy: 0.3343558282208589 \n",
      " error rate: 0.4437627811860941 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_coh = knn_coh.predict(X_test2)\n",
    "adj_pred_coh = result(pred_coh)\n",
    "\n",
    "print(\"accuracy range: %s \\n accuracy: %s \\n error rate: %s \\n\" \n",
    "    % (accuracy_range(y_test2, pred_coh), accuracy(adj_pred_coh, y_test2), error_rate(y_test2, adj_pred_coh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(knn_coh, open('knn_cohesion.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61b4062b24dfb1010f420dad5aa3bd73a4d2af47d0ec44eafec465a35a9d7239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
